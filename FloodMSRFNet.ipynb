{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fASDIL0tyUXK",
        "outputId": "acdba3d0-3d07-44d5-bd71-2390b8878b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM8YBj00IKCw",
        "outputId": "364206ca-f8cf-41f5-b3d3-c5c6080834c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Sep 11 16:38:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhcQWk-qyxV_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation, Input, InputLayer, Flatten, Conv2D, AveragePooling2D, MaxPool2D, GlobalAveragePooling2D, SeparableConv2D, Conv1D, Conv3D, GlobalAveragePooling3D, AveragePooling3D, DepthwiseConv2D, Conv2DTranspose, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from tensorflow_addons.layers import StochasticDepth, GroupNormalization\n",
        "from tensorflow_addons.optimizers import AdamW, RectifiedAdam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "from keras_unet_collection import models as umodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3jkVr_TL6iJ"
      },
      "outputs": [],
      "source": [
        "features = np.load('/content/drive/MyDrive/Flood/features.npy').astype(np.float32)\n",
        "labels = np.load('/content/drive/MyDrive/Flood/labels.npy').astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qknIjdLmRjEo"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, labels.reshape(labels.shape[0], 512*512), train_size=0.8, shuffle=True)\n",
        "x_train, x_test = tf.constant(x_train), tf.constant(x_test)\n",
        "y_train, y_test = tf.constant(y_train), tf.constant(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l_n0t3bbRbz"
      },
      "outputs": [],
      "source": [
        "class SqueezeAndExcite(tf.keras.layers.Layer):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.channels = input_shape[-1]\n",
        "\n",
        "        self.pooling = GlobalAveragePooling2D()\n",
        "        self.dense_1 = Dense(self.n, activation='relu')\n",
        "        self.dense_2 =  Dense(self.channels, activation='sigmoid')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "\n",
        "        r = self.pooling(x)\n",
        "        r = self.dense_1(r, training=training)\n",
        "        r = self.dense_2(r, training=training)\n",
        "\n",
        "        r = tf.reshape(r, (-1, 1, 1, self.channels))\n",
        "\n",
        "        return x * r\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"n\": self.n,\n",
        "            \"channels\": self.channels,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"dense_1\": self.dense_1,\n",
        "            \"dense_2\": self.dense_2\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class ResidualSEBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation='swish', survival_prob=1, dropout=0, kernel_regularizer=None,  shortcut='identity', reduction=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.filters = filters\n",
        "        self.strides = np.array([strides, strides]) if type(strides) == int else np.array(strides)\n",
        "        self.activation = activation\n",
        "        self.survival_prob = survival_prob\n",
        "        self.dropout = dropout\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.shortcut = shortcut\n",
        "        self.shortcut_mapping = None\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        spatial_dim = np.array([input_shape[1], input_shape[2]])\n",
        "        channels = input_shape[-1]\n",
        "\n",
        "        self.batch_norm_1 = BatchNormalization()\n",
        "        self.activation_1 = Activation(self.activation)\n",
        "        self.conv_1 = Conv2D(self.filters//4, 1, padding='same', kernel_regularizer=self.kernel_regularizer)\n",
        "        \n",
        "        self.batch_norm_2 = BatchNormalization()\n",
        "        self.activation_2 = Activation(self.activation)\n",
        "        self.conv_2 =  Conv2D(self.filters//4, 3, strides=self.strides, padding='same', kernel_regularizer=self.kernel_regularizer)\n",
        "\n",
        "        self.batch_norm_3 = BatchNormalization()\n",
        "        self.activation_3 = Activation(self.activation)\n",
        "        self.conv_3 =  Conv2D(self.filters, 1, padding='same', kernel_regularizer=self.kernel_regularizer)\n",
        "\n",
        "        self.squeeze_and_excite = SqueezeAndExcite(self.filters//self.reduction)\n",
        "        \n",
        "        if channels != self.filters or self.strides.prod() != 1: \n",
        "          if self.shortcut == 'identity':\n",
        "            self.projection = AveragePooling2D(pool_size=self.strides, strides=self.strides, padding='same')\n",
        "            self.shortcut_mapping = lambda x : tf.pad(self.projection(x), [[0, 0], [0, 0], [0, 0], [0, self.filters - channels]])\n",
        "          if self.shortcut == 'projection':\n",
        "            self.projection = Conv2D(self.filters, 1, strides=self.strides, padding='same')\n",
        "            self.shortcut_mapping = lambda x : self.projection(x)\n",
        "        else:\n",
        "          self.projection = None\n",
        "          self.shortcut_mapping = lambda x : x\n",
        "        \n",
        "        if self.survival_prob != 1:\n",
        "          self.stochastic_depth = StochasticDepth(self.survival_prob)\n",
        "        else:\n",
        "          self.stochastic_depth = None\n",
        "        \n",
        "        if self.dropout != 0:\n",
        "          self.dropout_layer = Dropout(self.dropout)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        \n",
        "        r = self.batch_norm_1(x, training=training)\n",
        "        r = self.activation_1(r)\n",
        "        r = self.conv_1(r, training=training)\n",
        "        \n",
        "        r = self.batch_norm_2(r, training=training)\n",
        "        r = self.activation_2(r)\n",
        "        r = self.conv_2(r, training=training)\n",
        "\n",
        "        r = self.batch_norm_3(r, training=training)\n",
        "        r = self.activation_3(r)\n",
        "        r = self.conv_3(r, training=training)\n",
        "\n",
        "        r = self.squeeze_and_excite(r)\n",
        "\n",
        "        x = self.shortcut_mapping(x)\n",
        "\n",
        "        if self.dropout != 0:\n",
        "          x = self.dropout_layer(x)\n",
        "\n",
        "        if self.survival_prob != 1:\n",
        "          return self.stochastic_depth([x, r], training=training)\n",
        "\n",
        "        return x + r\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"filters\": self.filters,\n",
        "            \"strides\": self.strides,\n",
        "            \"activation\": self.activation,\n",
        "            \"survival_prob\": self.survival_prob,\n",
        "            \"kernel_regularizer\": self.kernel_regularizer,\n",
        "            \"shortcut\": self.shortcut,\n",
        "            \"projection\": self.projection,\n",
        "            \"shortcut_mapping\": self.shortcut_mapping,\n",
        "            \"reduction\": self.reduction,\n",
        "            \"batch_norm_1\": self.batch_norm_1,\n",
        "            \"activation_1\": self.activation_1,\n",
        "            \"conv_1\": self.conv_1,\n",
        "            \"batch_norm_2\": self.batch_norm_2,\n",
        "            \"activation_2\": self.activation_2,\n",
        "            \"conv_2\": self.conv_2,\n",
        "            \"batch_norm_3\": self.batch_norm_3,\n",
        "            \"activation_3\": self.activation_3,\n",
        "            \"conv_3\": self.conv_3,\n",
        "            \"squeeze_and_excite\": self.squeeze_and_excite,\n",
        "            \"stochastic_depth\": self.stochastic_depth,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjh2cVKROTRG"
      },
      "outputs": [],
      "source": [
        "activation = 'swish'\n",
        "factor = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzsdkIJXPTcs"
      },
      "outputs": [],
      "source": [
        "def encoder(filters, x, n=1, pooling=True):\n",
        "    for i in range(n):\n",
        "        x = ResidualSEBlock(filters)(x)\n",
        "    if pooling:\n",
        "      p = MaxPool2D(padding='same')(x)\n",
        "      return x, p\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHzzw8QbVTxn"
      },
      "outputs": [],
      "source": [
        "def CLR(x, kernel_size, channels):\n",
        "  x = Activation(activation)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv2D(channels, kernel_size, padding='same')(x)\n",
        "  return x\n",
        "\n",
        "def CLRI(x, kernel_size, channels):\n",
        "  x = Conv2D(channels, kernel_size, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activation)(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVGJ703qWX05"
      },
      "outputs": [],
      "source": [
        "def DSDF(xh, xl, k, w=0.4):\n",
        "  h_channels, l_channels = xh.shape[-1], xl.shape[-1]\n",
        "  \n",
        "  m1h = CLR(xh, 3, k)\n",
        "  m1l = CLR(xl, 3, k)\n",
        "\n",
        "  m1hd = Conv2D(k, 3, strides=2, padding='same')(m1h)\n",
        "  m1lu = Conv2DTranspose(k, 3, strides=2, padding='same')(m1l) \n",
        "  m2h = CLR(tf.concat([m1lu, m1h], axis=-1), 3, k)\n",
        "  m2l = CLR(tf.concat([m1hd, m1l], axis=-1), 3, k)\n",
        "\n",
        "  m2hd = Conv2D(k, 3, strides=2, padding='same')(m2h)\n",
        "  m2lu = Conv2DTranspose(k, 3, strides=2, padding='same')(m2l)\n",
        "  m3h = CLR(tf.concat([m2lu, m1h, m2h], axis=-1), 3, k)\n",
        "  m3l = CLR(tf.concat([m2hd, m1l, m2l], axis=-1), 3, k)\n",
        "\n",
        "  m3hd = Conv2D(k, 3, strides=2, padding='same')(m3h)\n",
        "  m3lu = Conv2DTranspose(k, 3, strides=2, padding='same')(m3l)\n",
        "  m4h = CLR(tf.concat([m3lu, m1h, m2h, m3h], axis=-1), 3, k)\n",
        "  m4l = CLR(tf.concat([m3hd, m1l, m2l, m3l], axis=-1), 3, k)\n",
        "\n",
        "  m4hd = Conv2D(k, 3, strides=2, padding='same')(m4h)\n",
        "  m4lu = Conv2DTranspose(k, 3, strides=2, padding='same')(m4l)\n",
        "  m5h = Conv2D(xh.shape[-1], 3, activation=activation, padding='same')(tf.concat([m4lu, m1h, m2h, m3h, m4h], axis=-1))\n",
        "  m5l = Conv2D(xl.shape[-1], 3, activation=activation, padding='same')(tf.concat([m4hd, m1l, m2l, m3l, m4l], axis=-1))\n",
        "\n",
        "  return xh + w*m5h, xl + w*m5l\n",
        "\n",
        "def MSRF(x1, x2, x3, x4, w=0.4):\n",
        "\n",
        "  r1, r2 = DSDF(x1, x2, int(16/factor))\n",
        "  r3, r4 = DSDF(x3, x4, int(64/factor))\n",
        "\n",
        "  r1, r2 = DSDF(r1, r2, int(16/factor))\n",
        "  r3, r4 = DSDF(r3, r4, int(64/factor))\n",
        "\n",
        "  r2, r3 = DSDF(r2, r3, int(32/factor))\n",
        "\n",
        "  r1, r2 = DSDF(r1, r2, int(16/factor))\n",
        "  r3, r4 = DSDF(r3, r4, int(64/factor))\n",
        "\n",
        "  r2, r3 = DSDF(r2, r3, int(32/factor))\n",
        "\n",
        "  r1, r2 = DSDF(r1, r2, int(16/factor))\n",
        "  r3, r4 = DSDF(r3, r4, int(64/factor))\n",
        "\n",
        "  return x1 + w*r1, x2 + w*r2, x3 + w*r3, x4 + w*r4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D45WuK-mfOJE"
      },
      "outputs": [],
      "source": [
        "def RBI(x):\n",
        "  channels = x.shape[-1]\n",
        "  r = CLRI(x, 3, channels)\n",
        "  r = CLRI(r, 3, channels)\n",
        "  return x + r\n",
        "\n",
        "def GC(s, x):\n",
        "  channels = s.shape[-1]\n",
        "  a = Conv2D(channels, 1, padding='same')(tf.concat([s, x], axis=-1))\n",
        "  a = Activation('sigmoid')(a)\n",
        "  s = RBI(s * a)\n",
        "\n",
        "  return s\n",
        "\n",
        "def shape_stream(x1, x2, x3, x4, img_gradients):\n",
        "  spatial_dim = x1.shape[1:3]\n",
        "  s = CLRI(x1, 1, x1.shape[-1])\n",
        "  s = RBI(s)\n",
        "  s = RBI(GC(s, tf.image.resize(x2, spatial_dim)))\n",
        "  s = RBI(GC(s, tf.image.resize(x3, spatial_dim)))\n",
        "  s = RBI(GC(s, tf.image.resize(x4, spatial_dim)))\n",
        "  bce_output = Conv2D(1, 1, activation='sigmoid', padding='same')(s)\n",
        "  output = CLRI(tf.concat([bce_output, img_gradients[0], img_gradients[1]], axis=-1), 1, 1)\n",
        "  return bce_output, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQSjTEkCUOU1"
      },
      "outputs": [],
      "source": [
        "def RB(x):\n",
        "  channels = x.shape[-1]\n",
        "  r = CLR(x, 3, channels)\n",
        "  r = CLR(r, 3, channels)\n",
        "  return x + r\n",
        "\n",
        "def decoder(d, msrf):\n",
        "\n",
        "  channels = msrf.shape[-1]\n",
        "  d_up = Conv2DTranspose(channels, 3, strides=2, padding='same')(d)\n",
        "  x = Conv2D(channels, 3, padding='same')(tf.concat([d_up, msrf], axis=-1))\n",
        "  spatial = Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
        "  se = SqueezeAndExcite(x.shape[-1]//4)(x)\n",
        "  att1 = (spatial + 1) * se\n",
        "\n",
        "  channels = d.shape[-1]\n",
        "  msrf_down = Conv2D(channels, 3, strides=2, padding='same')(msrf)\n",
        "  d2 = Conv2D(channels, 1, padding='same')(d)\n",
        "  x = Activation('relu')(d2 + msrf_down)\n",
        "  x = Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
        "  x = Conv2DTranspose(1, 3, strides=2, padding='same')(x)\n",
        "  att2 = msrf * x\n",
        "\n",
        "  d_up = Conv2DTranspose(msrf.shape[-1], 3, strides=2, padding='same')(d)\n",
        "  x = Conv2D(channels, 1, padding='same')(tf.concat([att1, att2, d_up], axis=-1))\n",
        "  x = RB(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5MwnElHwHN_"
      },
      "outputs": [],
      "source": [
        "def decoder(d, r):\n",
        "  channels = r.shape[-1]\n",
        "  x = Conv2DTranspose(channels, 3, strides=2, padding='same')(d)\n",
        "  x = Conv2D(channels, 3, padding='same')(tf.concat([x, r], axis=-1))\n",
        "  x = ResidualSEBlock(x.shape[-1])(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmD5salfZDz-"
      },
      "outputs": [],
      "source": [
        "w = 0.4\n",
        "\n",
        "x = inp = Input((512, 512, 2))\n",
        "\n",
        "\n",
        "x = Conv2D(4, 3, strides=2, padding='same')(x)\n",
        "x = Conv2D(8, 3, strides=2, padding='same')(x)\n",
        "\n",
        "\n",
        "\n",
        "x1, p1 = encoder(int(64/factor), x)\n",
        "x2, p2 = encoder(int(128/factor), p1)\n",
        "x3, p3 = encoder(int(256/factor), p2)\n",
        "x4 = encoder(int(512/factor), p3, pooling=False)\n",
        "\n",
        "x1, x2, x3, x4 = MSRF(x1, x2, x3, x4, w=w)\n",
        "\n",
        "bce_out, ss = shape_stream(x1, x2, x3, x4, img_gradients=tf.image.image_gradients(x))\n",
        "\n",
        "x3 = decoder(x4, x3)\n",
        "x2 = decoder(x3, x2)\n",
        "x1 = decoder(x2, x1)\n",
        "\n",
        "x = Conv2D(int(64/factor), 3, activation=activation, padding='same')(tf.concat([ss, x1], axis=-1))\n",
        "x = Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Conv2DTranspose(1, 3, strides=2, padding='same')(x)\n",
        "x = Conv2DTranspose(1, 3, strides=2, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "bce_out = Conv2DTranspose(1, 3, strides=2, padding='same')(bce_out)\n",
        "bce_out = Conv2DTranspose(1, 3, strides=2, activation='sigmoid', padding='same')(bce_out)\n",
        "\n",
        "\n",
        "bce_out = Flatten(name='bce_output')(bce_out)\n",
        "x = Flatten(name='output')(x)\n",
        "\n",
        "models = [Model(inputs=inp, outputs=[x, bce_out])]\n",
        "models[0].summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aloh8bUBN_SO"
      },
      "outputs": [],
      "source": [
        "class AdjustedBinaryCrossentropy(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    return self.loss(y_true, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "class AdjustedBinaryCrossentropy2(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "\n",
        "    y_true = tf.reshape(y_true, [-1, 512, 512, 1])\n",
        "    y_true = tf.cast(tf.reduce_sum(tf.image.sobel_edges(y_true), axis=-1) != 0, tf.float32)\n",
        "    y_true = tf.reshape(y_true, [-1, 512*512])\n",
        "\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    return self.loss(y_true, y_pred, sample_weight=sample_weight)\n",
        "    \n",
        "class LogCoshDice(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # self.loss = tf.keras.losses.LogCosh()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    dice = 1 - tf.reduce_mean((2 * tf.reduce_sum(y_true * y_pred, axis=-1) + 1)/ (tf.reduce_sum(y_true, axis=-1) + tf.reduce_sum(y_pred, axis=-1) + 1))\n",
        "    return tf.math.log(tf.cosh(dice) + 1e-7) \n",
        "\n",
        "class SoftDice(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # self.loss = tf.keras.losses.LogCosh()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    return tf.reduce_mean(1 - (2 * tf.reduce_sum(y_true * y_pred, axis=-1) + 1) / (tf.reduce_sum(y_true**2, axis=-1) + tf.reduce_sum(y_pred**2, axis=-1) + 1))\n",
        "\n",
        "class PowerJaccard(tf.keras.losses.Loss):\n",
        "  def __init__(self, p):\n",
        "    super().__init__()\n",
        "    self.p = p\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    inter = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "    return tf.reduce_mean(1 - (inter + 1e-7)/ (tf.reduce_sum(y_true**self.p, axis=-1) + tf.reduce_sum(y_pred**self.p, axis=-1) - inter + 1e-7))\n",
        "\n",
        "class AdjustedBinaryCrossentropyAndDiceCoeff(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss1 = tf.keras.losses.BinaryCrossentropy()\n",
        "    self.loss2 = LogCoshDice()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    return (self.loss1(y_true, y_pred) + self.loss2(y_true, y_pred)) / 2\n",
        "\n",
        "class AdjustedBinaryCrossentropyAndDiceCoeff2(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss1 = tf.keras.losses.BinaryCrossentropy()\n",
        "    self.loss2 = SoftDice()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    return (self.loss1(y_true, y_pred) + self.loss2(y_true, y_pred)) / 2\n",
        "\n",
        "class AdjustedBinaryCrossentropyAndDiceCoeff3(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss1 = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weight=None):\n",
        "    mask = tf.cast(y_true != 255, tf.float32)\n",
        "    y_true = y_true * mask\n",
        "    y_pred = y_pred * mask\n",
        "    inter = tf.reduce_sum(y_true * y_pred)\n",
        "    loss2 = inter / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + inter + 1e-7) / (16 * 512 * 512)\n",
        "    return (self.loss1(y_true, y_pred) + loss2) / 2\n",
        "\n",
        "def jaccard(y_true, y_pred):\n",
        "  mask = tf.cast(y_true != 255, tf.float32)\n",
        "  y_true = tf.cast((y_true * mask) > 0.5, tf.float32)\n",
        "  y_pred =  tf.cast((y_pred * mask) > 0.5, tf.float32)\n",
        "  \n",
        "  n_null = tf.reduce_sum(tf.cast(y_true == 255, tf.float32), axis=1)\n",
        "\n",
        "  inter = tf.reduce_sum(tf.cast(y_true == y_pred, tf.float32), axis=1)\n",
        "  return (inter - n_null) / (2*512*512 - inter - n_null)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhFI-c5YJUeD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "decay_epochs = 10\n",
        "epochs = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuviRAbweNUu",
        "outputId": "a2503ad9-8f2b-4615-8265-602f246c07ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "14/14 [==============================] - 8s 282ms/step - loss: 2.0121 - jaccard: 0.3978 - val_loss: 0.7718 - val_jaccard: 0.4675\n",
            "Epoch 2/80\n",
            "14/14 [==============================] - 3s 214ms/step - loss: 0.6166 - jaccard: 0.6492 - val_loss: 0.5291 - val_jaccard: 0.7737\n",
            "Epoch 3/80\n",
            "14/14 [==============================] - 3s 214ms/step - loss: 0.3415 - jaccard: 0.8468 - val_loss: 0.5712 - val_jaccard: 0.7592\n",
            "Epoch 4/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.2237 - jaccard: 0.8696 - val_loss: 0.4048 - val_jaccard: 0.8299\n",
            "Epoch 5/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1924 - jaccard: 0.8829 - val_loss: 0.4222 - val_jaccard: 0.8082\n",
            "Epoch 6/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1884 - jaccard: 0.8850 - val_loss: 1.4781 - val_jaccard: 0.2862\n",
            "Epoch 7/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.2026 - jaccard: 0.8795 - val_loss: 0.2288 - val_jaccard: 0.8888\n",
            "Epoch 8/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1835 - jaccard: 0.8876 - val_loss: 0.7586 - val_jaccard: 0.5265\n",
            "Epoch 9/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1680 - jaccard: 0.8936 - val_loss: 0.2556 - val_jaccard: 0.8672\n",
            "Epoch 10/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1684 - jaccard: 0.8957 - val_loss: 0.2475 - val_jaccard: 0.8725\n",
            "Epoch 11/80\n",
            "14/14 [==============================] - 3s 220ms/step - loss: 0.1611 - jaccard: 0.8982 - val_loss: 0.2307 - val_jaccard: 0.8760\n",
            "Epoch 12/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1577 - jaccard: 0.8996 - val_loss: 0.1815 - val_jaccard: 0.8973\n",
            "Epoch 13/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1562 - jaccard: 0.9009 - val_loss: 0.1722 - val_jaccard: 0.9006\n",
            "Epoch 14/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1501 - jaccard: 0.9046 - val_loss: 0.1707 - val_jaccard: 0.9006\n",
            "Epoch 15/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1515 - jaccard: 0.9025 - val_loss: 0.1699 - val_jaccard: 0.9011\n",
            "Epoch 16/80\n",
            "14/14 [==============================] - 3s 221ms/step - loss: 0.1507 - jaccard: 0.9032 - val_loss: 0.1685 - val_jaccard: 0.9020\n",
            "Epoch 17/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1544 - jaccard: 0.8996 - val_loss: 0.1654 - val_jaccard: 0.9031\n",
            "Epoch 18/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1560 - jaccard: 0.9001 - val_loss: 0.1633 - val_jaccard: 0.9034\n",
            "Epoch 19/80\n",
            "14/14 [==============================] - 3s 220ms/step - loss: 0.1445 - jaccard: 0.9068 - val_loss: 0.1601 - val_jaccard: 0.9039\n",
            "Epoch 20/80\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 0.1525 - jaccard: 0.8998 - val_loss: 0.1612 - val_jaccard: 0.9040\n",
            "Epoch 21/80\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 0.1463 - jaccard: 0.9042 - val_loss: 0.1626 - val_jaccard: 0.9043\n",
            "Epoch 22/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1491 - jaccard: 0.9031 - val_loss: 0.1633 - val_jaccard: 0.9044\n",
            "Epoch 23/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1474 - jaccard: 0.9036 - val_loss: 0.1647 - val_jaccard: 0.9044\n",
            "Epoch 24/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1463 - jaccard: 0.9066 - val_loss: 0.1665 - val_jaccard: 0.9045\n",
            "Epoch 25/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1533 - jaccard: 0.9007 - val_loss: 0.1678 - val_jaccard: 0.9046\n",
            "Epoch 26/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1558 - jaccard: 0.9009 - val_loss: 0.1685 - val_jaccard: 0.9047\n",
            "Epoch 27/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1521 - jaccard: 0.9009 - val_loss: 0.1684 - val_jaccard: 0.9050\n",
            "Epoch 28/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1493 - jaccard: 0.9025 - val_loss: 0.1671 - val_jaccard: 0.9054\n",
            "Epoch 29/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1508 - jaccard: 0.9031 - val_loss: 0.1670 - val_jaccard: 0.9056\n",
            "Epoch 30/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1552 - jaccard: 0.8988 - val_loss: 0.1667 - val_jaccard: 0.9058\n",
            "Epoch 31/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1468 - jaccard: 0.9051 - val_loss: 0.1664 - val_jaccard: 0.9059\n",
            "Epoch 32/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1463 - jaccard: 0.9056 - val_loss: 0.1662 - val_jaccard: 0.9060\n",
            "Epoch 33/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1527 - jaccard: 0.9006 - val_loss: 0.1659 - val_jaccard: 0.9061\n",
            "Epoch 34/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1472 - jaccard: 0.9034 - val_loss: 0.1654 - val_jaccard: 0.9061\n",
            "Epoch 35/80\n",
            "14/14 [==============================] - 3s 221ms/step - loss: 0.1475 - jaccard: 0.9037 - val_loss: 0.1649 - val_jaccard: 0.9062\n",
            "Epoch 36/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1458 - jaccard: 0.9038 - val_loss: 0.1643 - val_jaccard: 0.9063\n",
            "Epoch 37/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1442 - jaccard: 0.9075 - val_loss: 0.1640 - val_jaccard: 0.9064\n",
            "Epoch 38/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1500 - jaccard: 0.9013 - val_loss: 0.1636 - val_jaccard: 0.9065\n",
            "Epoch 39/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1476 - jaccard: 0.9031 - val_loss: 0.1630 - val_jaccard: 0.9066\n",
            "Epoch 40/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1503 - jaccard: 0.9027 - val_loss: 0.1625 - val_jaccard: 0.9066\n",
            "Epoch 41/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1481 - jaccard: 0.9034 - val_loss: 0.1620 - val_jaccard: 0.9067\n",
            "Epoch 42/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1443 - jaccard: 0.9064 - val_loss: 0.1616 - val_jaccard: 0.9067\n",
            "Epoch 43/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1534 - jaccard: 0.9002 - val_loss: 0.1612 - val_jaccard: 0.9068\n",
            "Epoch 44/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1485 - jaccard: 0.9041 - val_loss: 0.1608 - val_jaccard: 0.9069\n",
            "Epoch 45/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1472 - jaccard: 0.9042 - val_loss: 0.1605 - val_jaccard: 0.9069\n",
            "Epoch 46/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1459 - jaccard: 0.9041 - val_loss: 0.1603 - val_jaccard: 0.9070\n",
            "Epoch 47/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1458 - jaccard: 0.9054 - val_loss: 0.1601 - val_jaccard: 0.9070\n",
            "Epoch 48/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1479 - jaccard: 0.9046 - val_loss: 0.1599 - val_jaccard: 0.9070\n",
            "Epoch 49/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1460 - jaccard: 0.9050 - val_loss: 0.1597 - val_jaccard: 0.9071\n",
            "Epoch 50/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1535 - jaccard: 0.8997 - val_loss: 0.1595 - val_jaccard: 0.9072\n",
            "Epoch 51/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1456 - jaccard: 0.9052 - val_loss: 0.1593 - val_jaccard: 0.9072\n",
            "Epoch 52/80\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 0.1451 - jaccard: 0.9058 - val_loss: 0.1593 - val_jaccard: 0.9072\n",
            "Epoch 53/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1474 - jaccard: 0.9041 - val_loss: 0.1591 - val_jaccard: 0.9072\n",
            "Epoch 54/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1520 - jaccard: 0.9015 - val_loss: 0.1591 - val_jaccard: 0.9072\n",
            "Epoch 55/80\n",
            "14/14 [==============================] - 3s 222ms/step - loss: 0.1485 - jaccard: 0.9029 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 56/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1427 - jaccard: 0.9066 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 57/80\n",
            "14/14 [==============================] - 3s 220ms/step - loss: 0.1519 - jaccard: 0.9000 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 58/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1452 - jaccard: 0.9064 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 59/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1485 - jaccard: 0.9036 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 60/80\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 0.1473 - jaccard: 0.9042 - val_loss: 0.1590 - val_jaccard: 0.9072\n",
            "Epoch 61/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1454 - jaccard: 0.9050 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 62/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1464 - jaccard: 0.9052 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 63/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1462 - jaccard: 0.9045 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 64/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1537 - jaccard: 0.8997 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 65/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1512 - jaccard: 0.9026 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 66/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1491 - jaccard: 0.9028 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 67/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1518 - jaccard: 0.9010 - val_loss: 0.1589 - val_jaccard: 0.9072\n",
            "Epoch 68/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1457 - jaccard: 0.9049 - val_loss: 0.1589 - val_jaccard: 0.9072\n",
            "Epoch 69/80\n",
            "14/14 [==============================] - 3s 216ms/step - loss: 0.1472 - jaccard: 0.9037 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 70/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1447 - jaccard: 0.9057 - val_loss: 0.1589 - val_jaccard: 0.9071\n",
            "Epoch 71/80\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 0.1485 - jaccard: 0.9041 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 72/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1491 - jaccard: 0.9039 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 73/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1485 - jaccard: 0.9030 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 74/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1475 - jaccard: 0.9044 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 75/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1485 - jaccard: 0.9027 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 76/80\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 0.1511 - jaccard: 0.9024 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 77/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1511 - jaccard: 0.9013 - val_loss: 0.1589 - val_jaccard: 0.9072\n",
            "Epoch 78/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1479 - jaccard: 0.9046 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 79/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1481 - jaccard: 0.9045 - val_loss: 0.1588 - val_jaccard: 0.9072\n",
            "Epoch 80/80\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 0.1461 - jaccard: 0.9049 - val_loss: 0.1588 - val_jaccard: 0.9072\n"
          ]
        }
      ],
      "source": [
        "for i, model in enumerate(models):\n",
        "\n",
        "  opt = Adam(tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=(x_train.shape[0]//batch_size)*decay_epochs, decay_rate=0.1, staircase=True))\n",
        "  loss = {\n",
        "      'output': AdjustedBinaryCrossentropy(),\n",
        "      # 'bce_output': AdjustedBinaryCrossentropy2()\n",
        "  }\n",
        "  metrics = {\n",
        "      'output': jaccard,\n",
        "  }\n",
        "\n",
        "  model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
        "  model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs , validation_data=(x_test, y_test))\n",
        "\n",
        "  # model.save(f'/content/drive/MyDrive/Flood/model_{i+1+num_saved}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcqaMBLavJ74"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
